{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch函数介绍\n",
    "本节参考《Python深度学习——基于PyTorch》代码和PyTorch官方文档 https://pytorch.org/tutorials/\n",
    "\n",
    "PyTorch官方网页中Docs栏有函数的介绍，Tutorails栏有简单教程，是很好的参考和学习资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PyTorch提供了一些基本的tensor操作，不仅能对tensor进行运算，也可以查看tensor对象的各种属性，本节主要介绍一些常用的运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2084,  0.8439],\n",
      "        [ 0.5176, -0.2377]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022) # 设置随机化种子\n",
    "# 创建随机矩阵\n",
    "r = (torch.rand(2, 2) - 0.5) * 2 # 生成值在 -1 和 1 之间的随机矩阵\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2084, 0.8439],\n",
      "        [0.5176, 0.2377]])\n"
     ]
    }
   ],
   "source": [
    "# 取绝对值\n",
    "print(torch.abs(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0., 1.],\n",
      "        [1., -0.]])\n",
      "tensor([[-0., 1.],\n",
      "        [1., -0.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 四舍五入\n",
    "print(torch.round(r))\n",
    "# 向上取整\n",
    "print(torch.ceil(r))\n",
    "# 向下取整\n",
    "print(torch.floor(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2099,  1.0045],\n",
      "        [ 0.5440, -0.2400]])\n",
      "tensor([[0.9784, 0.6646],\n",
      "        [0.8690, 0.9719]])\n",
      "tensor([ 0.,  1., -1.])\n"
     ]
    }
   ],
   "source": [
    "# 三角函数相关也是支持的\n",
    "# arcsin\n",
    "print(torch.asin(r))\n",
    "# cos\n",
    "print(torch.cos(r))\n",
    "# \n",
    "import math\n",
    "a = torch.tensor([0, math.pi / 4, 3 * math.pi / 4])\n",
    "print(torch.tan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3873)\n",
      "torch.return_types.svd(\n",
      "U=tensor([[ 0.8868, -0.4621],\n",
      "        [-0.4621, -0.8868]]),\n",
      "S=tensor([0.9573, 0.4046]),\n",
      "V=tensor([[-0.4429, -0.8966],\n",
      "        [ 0.8966, -0.4429]]))\n"
     ]
    }
   ],
   "source": [
    "# 计算方阵行列式\n",
    "print(torch.det(r))\n",
    "# 方阵SVD分解\n",
    "print(torch.svd(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5387)\n",
      "tensor(0.2288)\n"
     ]
    }
   ],
   "source": [
    "# 计算矩阵的统计量，例如均值、标准差等\n",
    "# 计算标准差和均值\n",
    "print(torch.std(r))\n",
    "print(torch.mean(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8439)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# 计算最大值、最小值\n",
    "print(torch.max(r))\n",
    "# 也可以计算argmax等\n",
    "print(torch.argmax(r))\n",
    "# 上面的操作都可以指定维度进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.8439],\n",
      "        [0.5176, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 限制值的范围\n",
    "print(torch.clamp(r, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a.shape)\n",
    "# 矩阵的转置\n",
    "b = a.T\n",
    "print(b.shape)\n",
    "# 改变张量形状\n",
    "c = a.view(-1, 1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([6, 4])\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([4, 2, 3])\n",
      "torch.Size([2, 1, 3, 4])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 1, 3, 1, 4])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3, 4)\n",
    "print(a.shape)\n",
    "# 改变张量形状\n",
    "b = a.reshape(-1, 4)\n",
    "print(b.shape)\n",
    "# 高维张量的转置（改变维度的顺序）\n",
    "b = a.transpose(0, 1)\n",
    "print(b.shape)\n",
    "# 改变高维张量维度的顺序\n",
    "b = a.permute(2, 0, 1)\n",
    "print(b.shape)\n",
    "# 插入新的维度\n",
    "b = a.unsqueeze(1)\n",
    "print(b.shape)\n",
    "# 去掉第二维\n",
    "b = a.squeeze(1)\n",
    "print(b.shape)\n",
    "# 插入新维度\n",
    "b = a.unsqueeze(1).unsqueeze(3)\n",
    "print(b.shape)\n",
    "# 去掉所有维数为1的维度\n",
    "b = b.squeeze()\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2) * 0.5\n",
    "b = torch.ones(2, 2) * 2.0\n",
    "print(a * b)    # 对应元素相乘\n",
    "print((torch.matmul(a, b))) # 矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 4, 5)\n",
    "b = torch.rand(3, 5, 6)\n",
    "# 张量中的矩阵分批次相乘\n",
    "c = torch.bmm(a, b)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8850, 0.0986],\n",
      "        [0.0030, 0.8304]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.2150, 0.5212],\n",
      "        [0.7944, 0.4251]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "print(a)\n",
    "# 生成和 a 形状相同的全零矩阵\n",
    "print(torch.zeros_like(a))\n",
    "# 生成和 a 形状相同的全一矩阵\n",
    "print(torch.ones_like(a))\n",
    "# 生成和 a 形状相同的随机矩阵\n",
    "print(torch.rand_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 类型转换\n",
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = a.double()\n",
    "print(b)\n",
    "\n",
    "c = a.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的复制\n",
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "# a 和 b 指向同一块内存\n",
    "a[0][1] = 561  \n",
    "print(b)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "# a 和 b 内容相同，但不是同一块内存\n",
    "print(torch.eq(a, b))   # 判断张量对应位置的值是否相等\n",
    "a[0][1] = 561\n",
    "print(b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7436, 0.0212],\n",
      "        [0.0590, 0.9506]], requires_grad=True)\n",
      "tensor([[0.7436, 0.0212],\n",
      "        [0.0590, 0.9506]], grad_fn=<CloneBackward>)\n",
      "tensor([[0.7436, 0.0212],\n",
      "        [0.0590, 0.9506]])\n",
      "tensor([[0.7436, 0.0212],\n",
      "        [0.0590, 0.9506]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True)\n",
    "print(a)\n",
    "# clone会保存计算图的信息\n",
    "b = a.clone()\n",
    "print(b)\n",
    "# detach可以将张量从计算图中剥离\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PyTorch张量求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(4., grad_fn=<MulBackward0>)\n",
      "tensor(12., grad_fn=<MulBackward0>)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2., requires_grad=True)\n",
    "print(a)\n",
    "b = a + 1.\n",
    "print(b)\n",
    "c = 2 * a\n",
    "print(c)\n",
    "d = b * c \n",
    "print(d)\n",
    "# PyTorch的计算图会记录计算的方式和对应的张量\n",
    "# 叶子节点grad_fn为None\n",
    "print(a.is_leaf)\n",
    "print(b.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(4.),)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 使用函数直接计算两个变量之间的梯度\n",
    "print(torch.autograd.grad(d, b))\n",
    "# 使用上面的函数计算梯度后，计算图会释放，因此再次运行该命令会报错\n",
    "# 但是张量不会保存梯度信息\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# 求梯度的第二种方式，会保存叶子节点的梯度信息\n",
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = a + 1.\n",
    "c = 2 * a\n",
    "d = b * c \n",
    "\n",
    "d.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.)\n"
     ]
    }
   ],
   "source": [
    "d1 = 3 * a\n",
    "d1.backward()\n",
    "# 梯度会累加\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "d2 = a * (a + 1)\n",
    "a.grad.data.zero_()\n",
    "d2.backward()\n",
    "print(a.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "114aa5d09031ee50efb3c0d5b4a15933e80f2a90cf94810cb1d979aa403edd3f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('hw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
